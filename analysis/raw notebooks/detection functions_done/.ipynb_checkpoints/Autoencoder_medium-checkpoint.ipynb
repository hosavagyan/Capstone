{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model, Sequential\n",
    "from keras import regularizers\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.32219047, 0.09950559, 0.        ],\n",
       "       [0.        , 0.32219047, 0.09950559, 0.        ],\n",
       "       [0.        , 0.32219047, 0.09950559, 0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.10170895, 0.0482553 , 0.        ],\n",
       "       [0.        , 0.10170895, 0.0482553 , 0.        ],\n",
       "       [0.        , 0.10170895, 0.0482553 , 0.        ]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_data = pd.read_csv('../data/Plant_1_Weather_Sensor_Data.csv')\n",
    "gen_data = pd.read_csv('../data/Plant_1_Generation_Data.csv')\n",
    "weather_data['DATE_TIME']= pd.to_datetime(weather_data['DATE_TIME'])\n",
    "weather_data['DAY'] = pd.DatetimeIndex(weather_data['DATE_TIME']).dayofyear\n",
    "weather_data['TIME'] = weather_data.DATE_TIME.dt.hour * 60 + weather_data.DATE_TIME.dt.minute\n",
    "weather_data['Time'] = weather_data.DATE_TIME.dt.time\n",
    "weather_data['HOUR'] = weather_data.DATE_TIME.dt.hour\n",
    "gen_data['DATE_TIME']= pd.to_datetime(gen_data['DATE_TIME'])\n",
    "merged_df = pd.merge(gen_data, weather_data, how='inner', on=['DATE_TIME'], suffixes=('', '_y'))\n",
    "X= merged_df[['DC_POWER', 'AMBIENT_TEMPERATURE', 'MODULE_TEMPERATURE', 'IRRADIATION']]\n",
    "X_scaled = MinMaxScaler().fit_transform(X)\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68774, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "619/619 [==============================] - 4s 5ms/step - loss: 0.0822 - val_loss: 0.0104\n",
      "Epoch 2/20\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0102 - val_loss: 0.0034\n",
      "Epoch 3/20\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0038 - val_loss: 0.0021\n",
      "Epoch 4/20\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0026 - val_loss: 0.0019\n",
      "Epoch 5/20\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0024 - val_loss: 0.0019\n",
      "Epoch 6/20\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0023 - val_loss: 0.0019\n",
      "Epoch 7/20\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0023 - val_loss: 0.0018\n",
      "Epoch 8/20\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0023 - val_loss: 0.0019\n",
      "Epoch 9/20\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0023 - val_loss: 0.0018\n",
      "Epoch 10/20\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0022 - val_loss: 0.0018\n",
      "Epoch 11/20\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0022 - val_loss: 0.0018\n",
      "Epoch 12/20\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0022 - val_loss: 0.0017\n",
      "Epoch 13/20\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0021 - val_loss: 0.0016\n",
      "Epoch 14/20\n",
      "283/619 [============>.................] - ETA: 0s - loss: 0.0021"
     ]
    }
   ],
   "source": [
    "input_layer = Input(shape =(X.shape[1], ))\n",
    "\n",
    "encoded = Dense(6, activation ='relu',activity_regularizer = regularizers.l1(10e-5))(input_layer)\n",
    "encoded = Dense(3, activation ='relu',activity_regularizer = regularizers.l1(10e-5))(encoded)\n",
    "output_layer = Dense(X.shape[1], activation ='sigmoid')(encoded)\n",
    "\n",
    "autoencoder = Model(input_layer, output_layer)\n",
    "autoencoder.compile(optimizer =\"adam\", loss =\"mse\")\n",
    "\n",
    "History=autoencoder.fit(X_scaled, X_scaled,batch_size = 100, epochs = 20,shuffle = True, validation_split = 0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(History.history['loss'], label='loss')\n",
    "plt.plot(History.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encoded_train_data=autoencoder.predict(X_scaled)\n",
    "scaled_train_features = pd.DataFrame(encoded_train_data, index=X.index, columns=X.columns)\n",
    "X_scaled_df=pd.DataFrame(X_scaled, index=X.index, columns=X.columns)\n",
    "train_mae_loss = np.mean((X_scaled_df - scaled_train_features)**2, axis=1)\n",
    "train_mae_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# plt.figure(figsize=(15, 10))\n",
    "plt.plot(scaled_train_features.DC_POWER, label='predicted')\n",
    "plt.plot(X_scaled_df.DC_POWER,  'r.', alpha=0.6,label='real')\n",
    "plt.title('predicted vs. real DC power of train data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_err=merged_df.join(pd.DataFrame(train_mae_loss))\n",
    "merged_err.rename(columns={0: 'ERROR'}, inplace=True)\n",
    "merged_err.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(merged_err['ERROR'])\n",
    "plt.title('Distribution of error values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "for i, inv in merged_err.groupby(merged_err.SOURCE_KEY):\n",
    "    if (i=='1BY6WEcLGh8j5v7')|(i=='bvBOhCH3iADSZry')|(i=='1IF53ai7Xc0U56Y')|(i=='uHbuxQJl8lW7ozc'):\n",
    "        plt.plot_date(inv.DATE_TIME, inv.ERROR,'.', label=i, alpha=0.7)\n",
    "#     plt.show()\n",
    "plt.legend()\n",
    "plt.title('Error values in time grouped by inverters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outliers_in_time(data, column, groupby_column, output_column_name=False, outlier_limit=3):\n",
    "    if output_column_name==False:\n",
    "        output_column_name=column+\"_outliers\"\n",
    "    data[output_column_name]=0\n",
    "    for i, group in  data.groupby(data[groupby_column]):\n",
    "        outlier_condition1=group[column]>(group[column].mean() + outlier_limit*group[column].std())\n",
    "        outlier_condition2=group[column]<(group[column].mean() - outlier_limit*group[column].std())\n",
    "        data.loc[group[outlier_condition1|outlier_condition2].index, output_column_name]=1\n",
    "    return data\n",
    "def get_outlier_errors(data, column, output_column_name=False, outlier_limit=3, window='7d'):\n",
    "    cleaned_column='cleaned'\n",
    "    if output_column_name==False:\n",
    "        output_column_name=column+\"_outliers\"\n",
    "    resorted=data.set_index('DATE_TIME').copy()\n",
    "    resorted[cleaned_column]=np.where(resorted[column]>0, resorted[column], np.nan)\n",
    "    resorted[output_column_name]=0\n",
    "    num=resorted.SOURCE_KEY.nunique()\n",
    "    outlier_condition1= resorted[cleaned_column]>resorted[cleaned_column].rolling(window=window, min_periods=num*8*4).mean()+outlier_limit*resorted[cleaned_column].rolling(window=window, min_periods=num*8*4).std()\n",
    "    resorted.loc[outlier_condition1, output_column_name]=1\n",
    "    return resorted.drop(columns=[cleaned_column]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_outliers=get_outlier_errors(merged_err, 'ERROR', output_column_name=\"alarm_error_outlier\", outlier_limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dc_outlier=get_outliers_in_time(merged_outliers, 'DC_POWER', 'HOUR', output_column_name=\"alarm_DC_conversion_outlier\", outlier_limit=3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def get_outliers_by_residual(data, x_column, y_column, output_column_name=\"residual_outliers\", anomaly_limit=5):\n",
    "    data[output_column_name]=0\n",
    "    for i in data.SOURCE_KEY.unique():\n",
    "        inv_data=data[data.SOURCE_KEY==i]\n",
    "        for a, day in inv_data.groupby(inv_data.DAY):\n",
    "            X = data[x_column].values.reshape(-1,1)\n",
    "            y = data[y_column].values.reshape(-1,1)\n",
    "            regressor = LinearRegression(fit_intercept=False)\n",
    "            regressor.fit(X, y)\n",
    "            m=regressor.coef_[0]\n",
    "            residual=(day[y_column]-m*day[x_column])**2 # or we can take the absolute value\n",
    "            mean_res = residual.mean()\n",
    "            stand_dev = residual.std()\n",
    "            data.loc[day[(residual>mean_res+anomaly_limit*stand_dev) | (residual<mean_res-anomaly_limit*stand_dev)].index, output_column_name] = 1\n",
    "    return data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "merged_ac_conv_out=get_outliers_by_residual(merged_dc_outlier, 'DC_POWER', 'AC_POWER',output_column_name=\"alarm_AC_conversion_outlier\", anomaly_limit=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plt.plot(merged_dc_outlier['DATE_TIME'], merged_dc_outlier['ERROR'], '.')\n",
    "plt.plot(merged_dc_outlier[merged_dc_outlier[\"alarm_error_outlier\"]==1]['DATE_TIME'], \n",
    "         merged_dc_outlier[merged_dc_outlier[\"alarm_error_outlier\"]==1]['ERROR'], 'r.')\n",
    "plt.plot(merged_dc_outlier[merged_dc_outlier[\"alarm_DC_conversion_outlier\"]==1]['DATE_TIME'], \n",
    "         merged_dc_outlier[merged_dc_outlier[\"alarm_DC_conversion_outlier\"]==1]['ERROR'], 'yx')\n",
    "#plt.plot(merged_ac_conv_out[merged_ac_conv_out[\"alarm_AC_conversion_outlier\"]==1]['DATE_TIME'], \n",
    "#         merged_ac_conv_out[merged_ac_conv_out[\"alarm_AC_conversion_outlier\"]==1]['ERROR'], 'gx')\n",
    "plt.xlabel('Date time')\n",
    "plt.ylabel('error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We can see that the high error rates are mainly caused by the errors that happen during Irradiation to DC and DC to AC conversions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inverter level efficiency\n",
    "#### a) Take the mean of the reconstruction error for each inverter after removing the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_outliers[merged_outliers.alarm_error_outlier == 1].shape)\n",
    "clean_data=merged_outliers.copy()\n",
    "print(clean_data.shape)\n",
    "clean_data.drop(clean_data.loc[clean_data.alarm_error_outlier == 1].index, inplace=True)\n",
    "print(clean_data.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverter_error(data):\n",
    "    mean_error=[]\n",
    "    inverters=[]\n",
    "    for i, inv in data.groupby(data.SOURCE_KEY):\n",
    "        ind = inv[inv['SOURCE_KEY']==i].index.values\n",
    "        m = inv.ERROR.mean()\n",
    "        mean_error.append(m)\n",
    "        inverters.append(i)\n",
    "    inv_error_df = pd.DataFrame(data=np.array(mean_error), columns=['MEAN_ERROR'])\n",
    "    source_key = pd.DataFrame(data=np.array(inverters), columns=['SOURCE_KEY'])\n",
    "    inv_error_df['SOURCE_KEY'] = source_key['SOURCE_KEY']\n",
    "    return(inv_error_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_error_df=inverter_error(clean_data)\n",
    "inv_error_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plt.scatter(inv_error_df.SOURCE_KEY,inv_error_df.MEAN_ERROR)\n",
    "plt.xticks(rotation=90) \n",
    "plt.ylim(inv_error_df.MEAN_ERROR.min()-0.0001,inv_error_df.MEAN_ERROR.max()+0.0001)\n",
    "plt.xlabel('inverters')\n",
    "plt.ylabel('mean error')\n",
    "plt.title('mean error of each inverter')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Calculate a mean reconstruction error per inverter per day and then compare those mean values to find inverters with higher mean reconstruction error in a single day and then over several days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_inverter_error(data):\n",
    "    mean_error=[]\n",
    "    inverters=[]\n",
    "    days=[]\n",
    "    for a, day in data.groupby(data.DAY):\n",
    "        for i, inv in day.groupby(day.SOURCE_KEY):\n",
    "            ind = inv[inv['SOURCE_KEY']==i].index.values\n",
    "            m = inv.ERROR.mean()\n",
    "            mean_error.append(m)\n",
    "            inverters.append(i)\n",
    "            days.append(a)\n",
    "    inv_error_df = pd.DataFrame(data=np.array(mean_error), columns=['MEAN_ERROR'])\n",
    "    source_key = pd.DataFrame(data=np.array(inverters), columns=['SOURCE_KEY'])\n",
    "    Days = pd.DataFrame(data=np.array(days), columns=['DAY'])\n",
    "    inv_error_df['SOURCE_KEY'] = source_key['SOURCE_KEY']\n",
    "    inv_error_df['DAY']=Days['DAY']\n",
    "    return(inv_error_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "daily_inv_error_df=daily_inverter_error(clean_data)\n",
    "daily_inv_error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "for i, day in daily_inv_error_df.groupby(daily_inv_error_df.DAY):\n",
    "    plt.scatter(day.SOURCE_KEY, day.MEAN_ERROR, label=i)\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylim(daily_inv_error_df.MEAN_ERROR.min()-0.001,daily_inv_error_df.MEAN_ERROR.max()+0.001)\n",
    "plt.ylabel('error rate')\n",
    "plt.legend(loc=\"upper right\", title=\"Days\", bbox_to_anchor=(1.08, 1))\n",
    "plt.title('Daily mean error of each inverter')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "for i, inv in daily_inv_error_df.groupby(daily_inv_error_df.SOURCE_KEY):\n",
    "    plt.scatter(inv.DAY, inv.MEAN_ERROR, label=i)\n",
    "plt.ylim(daily_inv_error_df.MEAN_ERROR.min()-0.001,daily_inv_error_df.MEAN_ERROR.max()+0.001)\n",
    "plt.ylabel('error rate')\n",
    "plt.xlabel('days')\n",
    "plt.legend(loc=\"upper right\", title=\"Inverters\", bbox_to_anchor=(1.18, 1))\n",
    "plt.title('Mean error of each day grouped by inverters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inefficient_inverters_day(data, column, output_column_name, anomaly_limit = 2):\n",
    "    data[output_column_name] = 0\n",
    "    for _,d in data.groupby('DAY'):\n",
    "        col_mean = d[column].mean()\n",
    "        col_std = d[column].std()\n",
    "        data.loc[d[(d[column] > col_mean + anomaly_limit * col_std)].index,output_column_name] = 1\n",
    "    return data\n",
    "def get_inefficient_inverters_window(data, column, output_column_name, window=7):\n",
    "    data[output_column_name] = 0\n",
    "    for inv in data['SOURCE_KEY'].unique():\n",
    "        data.loc[data['SOURCE_KEY']==inv, output_column_name]=data[data['SOURCE_KEY']==inv][column].rolling(window = window).min()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_inv_error_df = get_inefficient_inverters_day(daily_inv_error_df, 'MEAN_ERROR', 'alarm_inefficient_inverter_day')\n",
    "data=daily_inv_error_df\n",
    "for inv in data.SOURCE_KEY.unique():\n",
    "    plt.plot(data[data.SOURCE_KEY==inv]['DAY'], data[data.SOURCE_KEY==inv]['MEAN_ERROR'], 'b.')\n",
    "plt.plot(data[data.alarm_inefficient_inverter_day==1]['DAY'], data[data.alarm_inefficient_inverter_day==1]['MEAN_ERROR'], 'rx')\n",
    "plt.xlabel('day')\n",
    "plt.ylabel('DC conversion coefficient')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_inv_error_df[daily_inv_error_df['alarm_inefficient_inverter_day']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
